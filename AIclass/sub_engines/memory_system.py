
import chromadb
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import Document

from AIclass.events_class.system_events import LogMessageEvent

import asyncio

class MemorySystem:
    def __init__(self, embed_model, system_event_queue, google_drive_db_path="/app/my_ai_memory/chroma_db"):
        #google_drive_db_path æ—¶é•¿æœŸè®°å¿†dbå‚¨å­˜çš„ä½ç½®
        self.db_path = google_drive_db_path

        # embed_model to deal with memory search
        self.embed_model = embed_model

        def init_index():
            db = chromadb.PersistentClient(path=self.db_path)
            chroma_collection = db.get_or_create_collection("long_term_memory")
            #å¤šè¡Œæ•°åˆ—ï¼Œç¬¬ä¸€è¡Œæ˜¯æ–‡æ¡£ï¼Œç¬¬ä¸‰è¡Œæ˜¯å¯¹åº”çš„å‘é‡

            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
            #ä¸€ä¸ªåŸºäºä¸Šè¿°é›†åˆcollectionçš„ä¹¦æ¶

            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            #ä¹¦æ¶çš„æ³¨å†Œç³»ç»Ÿ
            # åˆ›å»ºå‘é‡ç´¢å¼•ï¼Œè¿™æ˜¯LlamaIndexä¸­æ“ä½œè®°å¿†çš„æ ¸å¿ƒ
            # å¦‚æœæ•°æ®åº“ä¸­å·²æœ‰è®°å¿†ï¼Œå®ƒä¼šè‡ªåŠ¨åŠ è½½

            index = VectorStoreIndex.from_vector_store(
            vector_store,
            embed_model=self.embed_model,
            )

            return index

        self.index = init_index()# å…ˆè®¾ä¸ºNone,embedding_modelä¸‹è½½å¥½ä¹‹åå†æ”¹å›æ¥

        # to log
        self.system_event_queue = system_event_queue

    async def init_and_get_index(self):

        "create vector_store, storage_contex and index, then return index"

        db = chromadb.PersistentClient(path=self.db_path)
        chroma_collection = db.get_or_create_collection("long_term_memory")
        #å¤šè¡Œæ•°åˆ—ï¼Œç¬¬ä¸€è¡Œæ˜¯æ–‡æ¡£ï¼Œç¬¬ä¸‰è¡Œæ˜¯å¯¹åº”çš„å‘é‡

        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        #ä¸€ä¸ªåŸºäºä¸Šè¿°é›†åˆcollectionçš„ä¹¦æ¶

        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        #ä¹¦æ¶çš„æ³¨å†Œç³»ç»Ÿ
        # åˆ›å»ºå‘é‡ç´¢å¼•ï¼Œè¿™æ˜¯LlamaIndexä¸­æ“ä½œè®°å¿†çš„æ ¸å¿ƒ
        # å¦‚æœæ•°æ®åº“ä¸­å·²æœ‰è®°å¿†ï¼Œå®ƒä¼šè‡ªåŠ¨åŠ è½½


        index = VectorStoreIndex.from_vector_store(
            vector_store,
            embed_model=self.embed_model,
        )
        #sä¹¦æ¶çš„ç®¡ç†å‘˜ï¼Œ  storage_context,indexéƒ½è‡ªåŠ¨å…³è”åˆ°åŒä¸€å¯¹è±¡ï¼Œ
        # å½“indexè¢«è°ƒç”¨ä¿®æ”¹æ—¶storage_ã€‚ã€‚è¢«è‡ªåŠ¨ä¿®æ”¹
        return index

    async def memorize(self, text_to_remember):
        """
        MemorySystemä½œç”¨çš„åœ°æ–¹3/3 

        # å°†æ–°çš„æ–‡æœ¬ä¿¡æ¯å­˜å…¥é•¿æœŸè®°å¿†
        # :param text_to_remember: éœ€è¦è¢«è®°ä½çš„å­—ç¬¦ä¸²
        # """


        # LlamaIndexéœ€è¦å°†æ–‡æœ¬åŒ…è£…æˆDocumentå¯¹è±¡
        document = Document(text=text_to_remember)
        self.index.insert(document)
        print(f"ğŸ§  æ–°è®°å¿†å·²å­˜å…¥: '{text_to_remember}'")

if __name__ == "__main__":
    from llama_index.embeddings.ollama import OllamaEmbedding
    from llama_index.llms.ollama import Ollama
    
    embed_model = OllamaEmbedding(model_name="bge-m3", base_url="http://localhost:11434")##
    system_event_queue = asyncio.Queue()
    ai_memory = MemorySystem(embed_model=embed_model, system_event_queue=system_event_queue)
    asyncio.run(ai_memory.memorize("åˆéŸ³æœªæ¥åˆå«mikuï¼Œæ˜¯æ—¥æœ¬è‘—åçš„è™šæ‹Ÿæ­Œå§¬"))
    





