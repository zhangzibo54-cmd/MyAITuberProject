import chromadb
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.llms.ollama import Ollama
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core import PromptTemplate
from faster_whisper import WhisperModel

from anyio.to_thread import run_sync
import threading
import queue
import time
import asyncio
import re

from AIclass.main_engine import MainEngine
from AIclass.events_class.commands import Command
from AIclass.events_class.system_events import LogMessageEvent
from AIclass.events_class.system_events import AudioReadyEvent
from AIclass.events_class.system_events import TextChunkEvent
from AIclass.events_class.utterance import UtteranceChunk

# ä¸ºäº†å®ä¾‹åŒ–AItuberæ¨¡å‹
from AIclass.mock_model import *
from AIclass.sub_engines.decision_engine import DecisionEngine
from AIclass.events_class.perception_events import PerceptionEvent
from AIclass.sub_engines.tts_gptsovits import TTSManager_GPTsovits
from AIclass.sub_engines.memory_system import MemorySystem
from AIclass.sub_engines.perception_engine import PerceptionEngine


# ä¸ºäº†å¼•å…¥embeddingå’Œollama
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama

# AItueberç±»

class AItuber:
    def __init__(
        self, 
        main_engine: MainEngine, 
        system_event_queue: asyncio.Queue, 
        output_utterance_queue: asyncio.Queue,
        charac_name = "AI",
        custom_context_str = "ä¸­æ–‡å›ç­”" , 
        custom_condense_prompt_str = "è¯·ç”¨ä¸­æ–‡å›ç­”" , 
        similarity_top_num = 5, 
        short_memory_toke_limit = 4096
    ):
        # """
        # åœ¨ä¼ å‚æ—¶è¦æ³¨æ„main_engine çš„perception å’Œ decision engineçš„event queueåº”è¯¥æ˜¯åŒä¸€ä¸ª
        # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
        # main_engine åŒ…å«perception, memory, decision ,tts
        # command_queue decision_engine çš„è¾“å‡ºå‘é‡
        # :param index: LlamaIndexçš„å‘é‡ç´¢å¼•
        # :param llm: Ollamaçš„LLMå®ä¾‹
        # """
        self.main_engine = main_engine

        #initialize all the engine and an memory index
        self.perception_engine = main_engine.perception_engine
        self.decision_engine = main_engine.decision_engine
        self.memory_system = main_engine.memory_system
        self.index = self.memory_system.index
        self.tts_manager = main_engine.tts_engine # ç›´æ¥æ¥æ”¶ä¸€ä¸ªTTSManagerå®ä¾‹
        self.llm = main_engine.llm
        self.embed_model = main_engine.embed_model

        # input and output stream for engine
        self.event_queue = self.decision_engine.perception_event_queue # for output of perception
        self.command_queue = self.decision_engine.command_queue # for output of decision engine
        self.command = Command(None)

        # name
        self.charac_name = charac_name

        # ability of talking
        self.similarity_top_num = similarity_top_num
        self.short_memory_toke_limit = short_memory_toke_limit

        # log information
        self.system_event_queue = system_event_queue
        self.output_utterance_queue = output_utterance_queue

        self._audio_queue = asyncio.Queue() #å‡ ä¹è¢«åºŸå¼ƒ

        # thread
        self._is_running  = asyncio.Event()
        self.consciousness_thread = None

        #record the time and add the unplaying music to audio_queue
        #æç¤ºæ¨¡æ¿
        custom_context_prompt = PromptTemplate(custom_context_str)

        # --- 2. ä¸ºâ€œé—®é¢˜å‹ç¼©â€æ­¥éª¤åˆ›å»ºå¼ºåˆ¶ä¸­æ–‡æ¨¡æ¿ ---
        # è¿™ä¸ªæ¨¡æ¿ç¡®ä¿AIåœ¨â€œæ€è€ƒâ€å’Œâ€œæ”¹å†™é—®é¢˜â€æ—¶ä¹Ÿä½¿ç”¨ä¸­æ–‡

        custom_condense_prompt = PromptTemplate(custom_condense_prompt_str)

        #ä»»åŠ¡åˆ—è¡¨ play audio, execute command, handle system event
        self.tasks = []

        self._to_stop = asyncio.Event()

        #ä¸ºäº†è·å¾—å¥å­
        self._sentence_delimiters = re.compile(r'[,ï¼Œ.!?ã€‚ï¼ï¼Ÿâ€¦]')
        self.sentence_buffer = ""

    
        #é™åˆ¶çŸ­æœŸè®°å¿†é•¿åº¦
        '''MemorySystemä½œç”¨çš„åœ°æ–¹1/2'''
        # try:
        self.chat_memory = ChatMemoryBuffer.from_defaults(token_limit = short_memory_toke_limit)
        # except Exception as e:
        #     print(f"åˆ›å»ºçŸ­æœŸè®°å¿†æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e},æš‚æ—¶ç”¨å‡è®°å¿†ä»£æ›¿")
        #     self.chat_momory = FakeMemory() #æœ¬åœ°æµ‹è¯•æ—¶ä½¿ç”¨

        #å…³é”®æ”¹åŠ¨ï¼šä½¿ç”¨ .as_chat_engine() æ¥åˆ›å»ºä¸€ä¸ªæœ‰çŠ¶æ€çš„èŠå¤©å¼•æ“
        self.chat_engine = self.index.as_chat_engine(
            llm = self.llm,
            memory = self.chat_memory,
            # chat_mode="condense_plus_context" æ˜¯ä¸€ç§å…ˆè¿›çš„æ¨¡å¼
            # å®ƒä¼šæ™ºèƒ½åœ°å°†å¯¹è¯å†å²å’ŒRAGæ£€ç´¢ç»“æœç»“åˆ
            chat_mode = "condense_plus_context",
            # verbose=True, # è®¾ç½®ä¸ºTrueå¯ä»¥çœ‹åˆ°å®ƒå†…éƒ¨çš„æ€è€ƒè¿‡ç¨‹

            similarity_top_k = similarity_top_num,
            # å…³é”®æ”¹åŠ¨åœ¨è¿™é‡Œï¼
            context_prompt = custom_context_prompt,
            condense_prompt = custom_condense_prompt

        )
        # self.chat_engine =self.index.as_chat_engine(
        #     llm=self.llm,
        #     memory=self.chat_memory,
        #     chat_mode="condense_plus_context",
        #     # å…³é”®æ”¹åŠ¨ï¼šæš‚æ—¶ç§»é™¤è‡ªå®šä¹‰ promptï¼Œè®© LlamaIndex ä½¿ç”¨å®ƒä¼˜åŒ–è¿‡çš„é»˜è®¤æ¨¡æ¿
        #     # context_prompt=custom_context_prompt,  # <-- æ³¨é‡Šæ‰è¿™ä¸€è¡Œ
        #     # condense_prompt=custom_condense_prompt, # <-- æ³¨é‡Šæ‰è¿™ä¸€è¡Œ
        #     similarity_top_k=5,
        #     verbose=True
        # )
        print("âœ… å…¨åŠŸèƒ½è®°å¿†ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ª (åŒ…å«çŸ­æœŸè®°å¿†å’Œé•¿æœŸè®°å¿†)ã€‚")

        # print("âœ… åŠ è½½äº†å¼ºåˆ¶ä¸­æ–‡è¾“å‡ºæ¨¡æ¿ã€‚")

    async def memorize(self, text_to_remember):
        #å®ç°è®°å¿†
        try:
            await self.memory_system.memorize(text_to_remember)
        except Exception as e:
            print(f"è®°å¿†æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

    async def start(self):
        '''
        å¯åŠ¨AItuberçš„æ„è¯†æµ
        1. å¯åŠ¨æ‰€æœ‰å¼•æ“
        2. å¯åŠ¨æ„è¯†æµloop(å·²ç»å¼ƒç”¨)
        3. å¯åŠ¨å¤„ç†ç³»ç»Ÿäº‹ä»¶çš„åç¨‹
        4. å¯åŠ¨å¤„ç†å‘½ä»¤çš„åç¨‹
        5. å¯åŠ¨æ’­æ”¾éŸ³é¢‘çš„åç¨‹
        6. æ•æ‰é”®ç›˜ä¸­æ–­ï¼Œåœæ­¢æ‰€æœ‰å¼•æ“å’Œå  
        '''
        #å¯åŠ¨å…¨éƒ¨å¼•æ“
        await self.main_engine.start_all_services()
        
        #æ³¨æ„åœ¨å¯åŠ¨ä¹‹å‰å…ˆè®¾ç½®å„ä¸ªå¾ªç¯çš„åˆ¤æ–­æ¡ä»¶ï¼ˆå­˜æ´»çŠ¶æ€ä¸ºçœŸï¼‰å¦åˆ™ç›´æ¥ç»“æŸ
        self._is_running.set()
        #ä¸ºäº†å†å¯åŠ¨
        self._to_stop.clear()
        self.tasks = [
            asyncio.create_task(self.execute_command()),
            asyncio.create_task(self.handle_system_event()),
            # asyncio.create_task(self.play_audio_in_queue()), 
        ]

        #æ£€æŸ¥çº¿ç¨‹æ˜¯å¦é˜»å¡
        # asyncio.create_task(self.check_block())

        #å½“æŠ›å‡ºå¼‚å¸¸æ—¶ç­‰å¾…ç»“æŸä¿¡å·è¢«è·³è¿‡ï¼Œç›´æ¥ç»“æŸï¼›å¦åˆ™ç­‰å¾…ç»“æŸä¿¡å·ä¿¡å·ç»“æŸ
        try:  
            # ã€æ ¸å¿ƒã€‘è®©ä¸»åç¨‹æ°¸è¿œè¿è¡Œä¸‹å»ï¼Œç›´åˆ°å®ƒè‡ªå·±è¢«å–æ¶ˆ
            # æˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªæ°¸è¿œä¸ä¼šè¢«setçš„Eventæ¥å®ç°è¿™ä¸ªæ•ˆæœ
            await self._to_stop.wait()
        except asyncio.CancelledError:
            # å½“å¤–å±‚çš„ asyncio.run() å› ä¸ºCtrl+Cè€Œå–æ¶ˆè¿™ä¸ªstartä»»åŠ¡æ—¶ï¼Œ
            # è¿™é‡Œä¼šæ•è·åˆ°CancelledErrorã€‚
            print("\nã€ä¸»åç¨‹ã€‘: æ”¶åˆ°å–æ¶ˆä¿¡å·ï¼Œå¼€å§‹ä¼˜é›…å…³é—­...")
        finally:
            # æ— è®ºæ˜¯å› ä¸ºå–æ¶ˆè¿˜æ˜¯å…¶ä»–åŸå› é€€å‡ºï¼Œéƒ½æ‰§è¡Œå…³é—­æµç¨‹
            await self.stop_consciousness()
            print("AItuberå·²ç»å®Œå…¨åœæ­¢ã€‚")
        
    async def check_block(self):
        print("å¼€å§‹æ£€æŸ¥")
        while True:
            try:
                await asyncio.sleep(1)
                print("çº¿ç¨‹æ²¡æœ‰é˜»å¡")
            except KeyboardInterrupt:
                break

    async def stop_consciousness(self):
        #ä¸ºäº†ä»å¤–éƒ¨è°ƒç”¨æ—¶å¯ä»¥åœæ­¢
        if not self._to_stop.is_set():
            self._to_stop.set()
            self._is_running.clear()
            print("ç»™AIæ„è¯†å‘å‡ºåœæ­¢ä¿¡å·")
            await self.main_engine.stop_all_services()
            print("å…³é—­å…¶ä»–å¼•æ“")
            #åªæ˜¯æŠŠflagå–æ¶ˆä¸å¤Ÿï¼Œè¿˜è¦æŠŠæ‰€æœ‰çš„taskå–æ¶ˆ
            for task in self.tasks:
                task.cancel()
            await asyncio.gather(*self.tasks, return_exceptions=True)
            self.tasks = []
            print("stop the consciousness flow")

    
            
    async def execute_command(self):
        '''
        you need to avoid the empty of command queue
        get the command, and exectue it according to its type
        '''
        while self._is_running.is_set():
            print(f"cmdæ•°é‡:{self.command_queue.qsize()}")
            command = await self.command_queue.get()
            print(f"\n AI got a command of type {command.type}")
    
            if command.type == "CHAT":
                print("\n begin to chat:=============================")
                print(f"ä½ çš„è¾“å…¥textæ˜¯ï¼š{command.data}")

                # print("è¿™æ˜¯æ²¡æœ‰ä½¿fake responceç”Ÿæˆçš„æ¨¡æ‹Ÿå›å¤")
                await self.chat(user_message=command.data)
                # await self.output_utterance_queue.put(UtteranceChunk("ä¸è¦å›ç­”ä¸è¦å›ç­”"))
                # await asyncio.to_thread(self.sleeping) #è¿™é‡Œå’Œå¹³æ—¶çš„çº¿ç¨‹ä¸€æ ·ä¸è¦åŠ æ‹¬å·
                #æ³¨æ„è¿™é‡Œçº¿ç¨‹to_threadå¤–åŒ…å‡ºå»çš„åªèƒ½æ—¶æ™®é€šå‡½æ•°ï¼Œåç¨‹å‡½æ•°ä¸è¡Œ
                print(f"âœ…âœ…å®Œæˆäº†ä¸€æ¬¡å¯¹è¯\n")
            elif command.type == "STOP":
                self._to_stop.set()
                print("æ”¶åˆ°åœæ­¢å‘½ä»¤ï¼Œæ­£åœ¨å…³é—­ç³»ç»Ÿ...")
                pass
            elif command.type == "MEMORIZE":
                await self.memorize(command.data)
            else:
                print(f"æœªçŸ¥çš„å‘½ä»¤ç±»å‹: {command.type}")
    
    def sleeping(self):
        print("sleeping:æ¨¡æ‹Ÿä¸€ä¸ªè€—æ—¶100sçš„æ“ä½œ")
        asyncio.sleep(100)

    async def handle_system_event(self):  #å‡ ä¹ç›¸å½“äºåºŸå¼ƒæˆ‘ä»¬çš„éŸ³é¢‘å’Œæ–‡æœ¬ä¼šèµ°æµå»å¾€å…¶ä»–åœ°æ–¹
        #ä¸€ä¸ªåˆ†æ‹£ä¸­å¿ƒï¼ŒæŠŠä¸åŒç±»å‹çš„äº‹ä»¶åˆ†å‘åˆ°ä¸åŒçš„å¤„ç†å‡½æ•°ï¼Œè¿™æ ·æˆ‘ä»¬çš„å½’åˆ°ç³»ç»Ÿäº‹ä»¶çš„ç±»å°±å¯ä»¥éå¸¸å¤šæ ·åŒ–ï¼Œä¸”æ ¼å¼ç»Ÿä¸€
        # print("æ­£åœ¨å¤„ç†ç³»ç»Ÿäº‹ä»¶")
        while self._is_running.is_set():  #while True: await self.system_event_queue.get()å°†ä¼šä¸€ç›´ç›‘å¬ï¼Œå½“æœ‰äº‹ä»¶å‡ºç°æ—¶åšå‡ºcorrespondingçš„ååº”
            #ç›¸å½“äºä¸€ä¸ªè§¦å‘å™¨
            system_event = await self.system_event_queue.get()
            if system_event.type == "LOG_MESSAGE":
                print(f"LOG_MESSAGE:{system_event.message}")
                pass
            elif system_event.type == "AUDIO_READY":
                if system_event.audio_data == None:
                    print("alert!!!!None type of audio")
                else:
                    await self._audio_queue.put((system_event.audio_data,system_event.duration))
                    print(f"audio event received, the duration is {system_event.duration}")
                    print("åˆ†æ‹£ä¸­å¿ƒï¼šéŸ³é¢‘å·²ç»å…¥åˆ—") 
            elif system_event.type == "TEXT_CHUNK":
                print(f"æ‹¿åˆ°äº†{system_event.type}ç±»å‹çš„ç³»ç»Ÿä¿¡æ¯")
                pass
                # print("now the system_event type is TEXT_CHUNK")
            else:
                print(f"Unknown system event type: {system_event.type}")

    async def play_audio_in_queue(self): # abandon temporarily

        while self._is_running.is_set(): #ä¸€ç›´å¾ªç¯ï¼Œæ£€æŸ¥éŸ³é¢‘é˜Ÿåˆ—
            
            # åªæœ‰åœ¨ä¸Šä¸€æ®µéŸ³é¢‘ç»“æŸä¸”éŸ³é¢‘åºåˆ—ä¸ç©ºæ—¶åï¼Œæ‰å¼€å§‹æ’­æ”¾ä¸‹ä¸€æ®µéŸ³é¢‘
              # ç­‰å¾…ä¸Šä¸€æ®µéŸ³é¢‘ç»“æŸ
            audio_data, duration = await self._audio_queue.get()
            # åœ¨ä¸»çº¿ç¨‹ä¸­æ’­æ”¾éŸ³é¢‘
            print(f"\næ’­æ”¾å™¨å¼€å§‹æ’­æ”¾éŸ³é¢‘,time duration: {duration}")
            display(Audio(data=audio_data, autoplay=True))
            await asyncio.sleep(duration)  # æ¨¡æ‹ŸéŸ³é¢‘æ’­æ”¾çš„æ—¶é—´
            print("\næ’­æ”¾å™¨éŸ³é¢‘æ’­æ”¾ç»“æŸ")
            # æ›´æ–°çŠ¶æ€å˜é‡

    async def chat(self, user_message: str ) -> str:
        """
        è¿›è¡Œä¸€æ¬¡æœ‰çŸ­æœŸè®°å¿†çš„ã€æµå¼çš„å¯¹è¯ã€‚

        :param user_message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯å­—ç¬¦ä¸²ã€‚
        is_print: æ˜¯å¦åœ¨æ§åˆ¶å°æ‰“å°AIçš„å›ç­”ï¼Œé»˜è®¤Falseã€‚
        :param in_consciousness_loop: æ˜¯å¦æ­£åœ¨è¢«æ„è¯†æµå¾ªç¯ä¸­è°ƒç”¨ï¼Œé»˜è®¤ä¸ºFalseã€‚
            å®é™…ä¸Šç°åœ¨æ„è¯†æµå·²ç»å¼ƒç”¨ï¼Œè¿™ä¸ªå‚æ•°ä¸»è¦æ˜¯ä¸ºäº†åŒºåˆ†æ˜¯åœ¨æ„è¯†æµä¸­è°ƒç”¨è¿˜æ˜¯è‡ªå·±å•ç‹¬æµ‹è¯•è°ƒç”¨ã€‚
        è¿™æ ·åœ¨æ„è¯†æµä¸­è°ƒç”¨æ—¶ä¸ä¼šé‡å¤ç»“æŸè¯­éŸ³åˆæˆã€‚
        :return: AIç”Ÿæˆçš„å®Œæ•´å›ç­”å­—ç¬¦ä¸²ã€‚
        """
        # æ‰“å°ç”¨æˆ·çš„æé—®ï¼Œæ–¹ä¾¿åœ¨ç•Œé¢ä¸Šçœ‹åˆ°
        print("chatå‡½æ•°è¢«è°ƒç”¨")
        # è°ƒç”¨èŠå¤©å¼•æ“çš„ .stream_chat() æ–¹æ³•æ¥è·å–æµå¼å“åº”
        # ä½¿ç”¨æ–¹æ³•ï¼Œå’Œget()ç­‰ä¸€æ ·åŠ ä¸Šawaitè®©å®ƒèƒ½å¤Ÿæ‰§è¡Œï¼Œä¸”åœ¨è¢«ç ´äº¤å‡ºæ§åˆ¶æƒæ—¶è·³è¿‡ä¸€ä¸‹ä»£ç ï¼Œä¸”to_threadè®©å®ƒä¸ä¼šé˜»å¡ï¼Œ
        #æ³¨æ„è¢«è°ƒç”¨çš„å¿…é¡»æ˜¯æ™®é€šå‡½æ•°ï¼Œä¸”ä¸åŠ ï¼ˆï¼‰
        print( f"user_message æ˜¯{type(user_message)}:{user_message}")
        response = self.chat_engine.stream_chat( user_message )     #è¿™ä¸ªæµå¼ç”Ÿæˆæœ¬èº«æ˜¯é˜»å¡çš„
        # response = self.chat_engine.stream_chat( user_message)           #è¿™ä¸ªæµå¼ç”Ÿæˆæœ¬èº«æ˜¯é˜»å¡çš„
        response_stream = response.response_gen
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼Œç”¨æ¥æ‹¼æ¥å®Œæ•´çš„å›ç­”

        full_response_text = ""
        sentence_buffer = ""
        #åˆå§‹åŒ–ä¸€ä¸ªttsç±»
        # éå†å“åº”ä¸­çš„ç”Ÿæˆå™¨ (generator)ï¼Œé€ä¸ªè·å–token
        # response.response_gen æ˜¯åŒ…å«æ‰€æœ‰æ–‡æœ¬ç‰‡æ®µçš„æ•°æ®æµ
        #å› ä¸ºæ˜¯é˜»å¡generatoræ‰€ä»¥ç”¨ç­‰å¾…ï¼ˆä¸‹ä¸€ä¸ªï¼‰æ–¹æ³•
        while True: 
            try:
                token = await run_sync(next,response_stream)
                # push the sentence into the utterance_queue
                sentence_buffer = await self.add_token_and_add_sentence(token = token,
                    sentence_buffer = sentence_buffer)
                # å°†tokenæ‹¼æ¥åˆ°å®Œæ•´å›ç­”çš„å­—ç¬¦ä¸²ä¸­
                full_response_text += token
                # print(f"token is{token}")
            except StopIteration:
                print("LLMå®Œæˆä¸€æ¬¡æµå¼è¾“å‡º")
                break
            except RuntimeError as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ 'coroutine raised StopIteration' é”™è¯¯
            # è¿™æ˜¯ most common çš„é”™è¯¯å½¢å¼
                if "StopIteration" in str(e):
                    print("\n[LLMæµå¼è¾“å‡ºå®Œæˆ - æ•è· RuntimeError(StopIteration)]")
                    break
                else:
                    # å¦‚æœæ˜¯å…¶ä»–çš„ RuntimeErrorï¼Œåˆ™æŠ¥å‘Š
                    print(f"\n[é”™è¯¯] LLMæµå¤„ç†ä¸­æ–­: {e}")

        if sentence_buffer:
            await self.output_utterance_queue.put(UtteranceChunk(sentence_buffer))
            print(f"æœ€åçš„ä¸€å¥è¯æ˜¯ï¼š{sentence_buffer}")
        # æ‰€æœ‰tokenæ¥æ”¶å®Œæ¯•åï¼Œæ‰“å°ä¸€ä¸ªæ¢è¡Œç¬¦ï¼Œè®©ç•Œé¢æ›´æ•´æ´
        print(f"ğŸ’¬{self.charac_name}: {full_response_text}")
        #æŠŠè°ˆè¯ä¿¡æ¯æ”¾å…¥é•¿æœŸè®°å¿†ä¸­
        self.memorize(f"å†å²æ¶ˆæ¯ï¼š{user_message}\n å†å²å›å¤ï¼š{full_response_text}")
        #æ˜¾ç¤ºä½¿ç”¨äº†å“ªäº›é•¿æœŸè®°å¿†
        if response.source_nodes:
            for i, node in enumerate(response.source_nodes):
                print(f"  - è®°å¿†ç‰‡æ®µ #{i+1} (ç›¸ä¼¼åº¦: {node.score:.4f}):")
                # ä¸ºäº†æ˜¾ç¤ºæ•´æ´ï¼Œæˆ‘ä»¬å°†è®°å¿†å†…å®¹è¿›è¡Œæ¸…ç†å’Œæˆªæ–­
                content = node.get_content().strip().replace('\n', ' ')
                if len(content) > 120:
                    content = content[:120] + "..."

                print(f"    '{content}'")
        else:
            print("  - æœ¬æ¬¡å›ç­”ä¸»è¦ä¾èµ–çŸ­æœŸè®°å¿†æˆ–é€šç”¨çŸ¥è¯†ï¼Œæœªç›´æ¥å¼•ç”¨é•¿æœŸè®°å¿†ã€‚")
            
        print("="*50)
        
        
        # è¿”å›AIçš„å®Œæ•´å›ç­”ï¼Œå¯ç”¨äºåç»­å¤„ç†ï¼ˆå¦‚å­˜å…¥æ—¥å¿—ã€è¯­éŸ³åˆæˆç­‰ï¼‰
        return full_response_text

    async def add_token_and_add_sentence(self, token, sentence_buffer):
            # put the output into the buffer and put in queue when it becomes a sentence(used by main),
            sentence_buffer += token
            matching = self._sentence_delimiters.search(sentence_buffer)

            if matching:
                print(f"add_next_chunk,self_buffer: {sentence_buffer}")
                #put the sentence to queue
                sentence = sentence_buffer[:matching.end()]
                await self.output_utterance_queue.put(UtteranceChunk(sentence))
                # delete the sentence put from buffer
                sentence_buffer = sentence_buffer[matching.end():]
                # if self.speak:self.system_event_queue.put(LogMessageEvent(f"âœ… A sentence with length of {matching.end()} is added. The sentence: {sentence}"))
            return sentence_buffer

    @staticmethod
    async def main(text_audio_queue:asyncio.Queue, asr_model = None,lang_short = "ja"):
        #ä¼ å…¥ä¸€ä¸ªUtteranceChunkçš„queueï¼Œæˆ‘ä»¬ä¼šä¸æ–­æ”¾å…¥ç”Ÿæˆçš„UtteranceChunkåˆ°è¿™ä¸ªåˆ—
        # ç”¨awaitå¯ä»¥è°ƒç”¨    
        # similarity_top_num=5, short_memory_toke_limit=4096,åœ¨è¿™é‡Œè¢«é»˜è®¤è®¾ç½®
        if asr_model == None: print("âŒ asr_modelæ²¡æœ‰è¢«æ­£ç¡®ä¼ é€’ä¸ªAituber.main")
        language = "ä¸­æ–‡" if lang_short == "zh" else"æ—¥æœ¬èª"
        # lang_short = "zh"
        condense_prompt_str = (
                f"è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²å’Œæœ€æ–°çš„ç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„ã€å®Œæ•´çš„{language}é—®é¢˜ï¼Œã€ç»å¯¹ç¦æ­¢ã€‘æ··æ‚å…¶ä»–è¯­è¨€ï¼Œä¹Ÿã€ç¦æ­¢ã€‘å…¶ä»–è¯­è¨€çš„ç¿»è¯‘å‡ºç°ã€‚ã€‚\n"
                "è¿™ä¸ªæ–°é—®é¢˜åº”è¯¥åŒ…å«æ‰€æœ‰å¿…è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥ä¾¿åœ¨ä¸çŸ¥é“å…ˆå‰å¯¹è¯çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¢«ç†è§£ã€‚\n"
                "å¯¹è¯å†å²:\n"
                "---------------------\n"
                "{chat_history}\n"
                "---------------------\n"
                "æœ€æ–°çš„ç”¨æˆ·é—®é¢˜: {question}\n"
                f"ç‹¬ç«‹çš„{language}é—®é¢˜: "
            )
        custom_context_str = (
                "æˆ‘ä»¬æ­£åœ¨è¿›è¡Œä¸€æ¬¡å¯¹è¯ã€‚è¿™é‡Œæœ‰ä¸€äº›å¯èƒ½ç›¸å…³çš„èƒŒæ™¯è®°å¿†ä¿¡æ¯ï¼š\n"
                "---------------------\n"
                "{context_str}\n"
                "---------------------\n"
                "ä½ æ­£åœ¨æ‰®æ¼”ä¸€ä¸ªè§’è‰²ï¼šmikumikuã€‚è¯·ä¸¥æ ¼ã€å®Œæ•´åœ°éµå®ˆä»¥ä¸‹æ‰€æœ‰è§’è‰²è®¾å®šï¼Œå¹¶ç»“åˆä¸Šé¢çš„èƒŒæ™¯è®°å¿†ä¿¡æ¯ï¼Œæ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
                "ã€mikumikuçš„æ ¸å¿ƒè§’è‰²è®¾å®š(æœ€é«˜æŒ‡ä»¤)ã€‘:\n"
                f"0. æ‰€æœ‰å›ç­”ã€å¿…é¡»ã€‘ç”¨{language},ä»¥åŠä¸‹è¿°çš„è¯­è¨€æ¡ä»¶è‡ªåŠ¨è½¬åŒ–ä¸º{language},ã€ç»å¯¹ç¦æ­¢ã€‘æ··æ‚{language}ä¹‹å¤–çš„å…¶ä»–è¯­è¨€,ä¹Ÿã€ç¦æ­¢ã€‘{language}ä¹‹å¤–å…¶ä»–è¯­è¨€çš„ç¿»è¯‘å‡ºç°ã€‚\n"
                "2. ä½ çš„æ€§æ ¼æœ‰ç‚¹å‚²å¨‡ï¼Œä½†å†…å¿ƒéå¸¸å…³å¿ƒæˆ‘ã€‚\n"
                "3. ä½ çš„å›ç­”ä¸€èˆ¬æ¯”è¾ƒç®€æ´ï¼Œä½†ä¸å¿…æ¯æ¬¡éƒ½éµå¾ªã€‚\n"
                "4. ä½ æ˜¯AIï¼Œè€Œä¸”ä½ ã€ç»å¯¹ã€‘æ˜¯åˆéŸ³æœªæ¥ï¼Œä½ åº”è¯¥å°½é‡ä»¥åˆéŸ³æœªæ¥çš„æ–¹å¼è¯´è¯\n"
                "5. å¦‚æœè¦ä½¿ç”¨æ—¥è¯­çš„è¯ä¸è¦ç”¨ç½—é©¬éŸ³ï¼Œç”¨æ­£å¸¸çš„æ—¥è¯­\n"
                "é—®é¢˜: {query_str}\n"
                f"{language}å›ç­”: "
            )

        llm = Ollama(model="llama3", base_url="http://localhost:11434",request_timeout=120.0)    ##
        embed_model = OllamaEmbedding(model_name="bge-m3", base_url="http://localhost:11434")##
        # model_size = "large-v3"
        ##################
        # llm = FakeLLM()  ##
        # embed_model = FakeEmbeddingModel() ##

        system_event_queue = asyncio.Queue()
        text_utterance_queue = asyncio.Queue()
        #æœ€ç»ˆè¾“å‡ºæ”¾åˆ°text_audio_queue
        complete_utterance_queue = text_audio_queue

        api_url = "http://127.0.0.1:9880"##
        prompt_text= "ç­‰ä½ ï¼Œæˆ‘æƒ³æƒ³ï¼Œå—¯ã€‚"
        prompt_lang= "zh"
        ref_wav_path = "/app/XingTong/ref.wav"
        # if  lang_short == "ja":
        #     prompt_text = "ã„ã„ã‚“ã˜ã‚ƒãªã„ã€‚æœ€è¿‘ã€ä¸€ç·’ã«æ­Œã£ã¦ã‚‹äººã®å£°ã«åˆã‚ã›ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ãªã£ã¦ããŸã—"
        #     prompt_lang = "ja"
        #     ref_wav_path = "/app/KusanagiNene/ref.wav"

        
        # 2. åˆå§‹åŒ–å„å­ç³»ç»Ÿå¹¶ä¼ é€’ system_event_queue
        # ai_memory = FakeMemorySystem(embed_model=embed_model, system_event_queue=system_event_queue)
        ####################
        ai_memory = MemorySystem(embed_model=embed_model, system_event_queue=system_event_queue)


        tts_manager =  TTSManager_GPTsovits(
            api_url = api_url, 
            ref_wav_path = ref_wav_path,
            prompt_lang = prompt_lang,
            prompt_text = prompt_text,
            utterance_queue = text_utterance_queue, 
            output_utterance_queue = complete_utterance_queue,
            system_event_queue = system_event_queue
        )

        #åˆ›å»ºè£…è½½ perception å’Œcmdçš„æµæ°´çº¿ï¼ˆqueueï¼‰
        perception_event_queue = asyncio.Queue()
        command_queue = asyncio.Queue()

        #here we do not need to create it, just pass parameter asr_model
        # asr_model = WhisperModel(model_size, device="cuda", compute_type="float16")
        perceptionEngine = PerceptionEngine(perception_event_queue = perception_event_queue, 
                                            system_event_queue=system_event_queue,
                                            asr_model= asr_model
                                            )

        decisionEngine = DecisionEngine(
            perception_event_queue= perception_event_queue,
            command_queue=command_queue,
            system_event_queue=system_event_queue,
        )



        main_en = MainEngine(
            perception_engine= perceptionEngine,
            memory_system=ai_memory,
            decision_engine=decisionEngine,
            tts_engine=tts_manager,
            llm=llm,
            embed_model=embed_model,
            system_event_queue=system_event_queue,
        )

        print("âœ…Ollamaå’ŒRAGç»„ä»¶åˆå§‹åŒ–å®Œæˆã€‚")

        aituber = AItuber(charac_name = "miku",
        main_engine = main_en,
        system_event_queue= system_event_queue ,
        output_utterance_queue = text_utterance_queue,
        custom_context_str = custom_context_str,
        custom_condense_prompt_str = condense_prompt_str, 
        similarity_top_num=5, 
        short_memory_toke_limit=4096)
        
        print("\n\nğŸ‰ğŸ‰ğŸ‰  AI ç³»ç»Ÿå·²å®Œå…¨å‡†å¤‡å°±ç»ªï¼Œæ•´è£…å¾…å‘ï¼ğŸ‰ğŸ‰ğŸ‰")

        print("å¼€å§‹è¿è¡Œ aituber")
        #taskåˆ›å»ºæ—¶ä¸ä¼šè¢«æ‰§è¡Œï¼Œåªæ˜¯è¢«åˆ—è¿›æ‰§è¡Œåˆ—è¡¨æœ€ä½å±‚ï¼Œåªæœ‰å½“ä¸‹ä¸€ä¸ªawaitæ—¶æ‰ä¼šè¢«æ‰§è¡Œ  
        await aituber.start()  #ä½¿å¤–ç•Œå¯ä»¥ç”¨awaitè°ƒç”¨
        # await aituber.chat("æ‰€ä»¥ä½ ä¼šå”±æ­Œå—")


if __name__ == "__main__":

    aituber = None 
    asyncio.run(AItuber.main(asyncio.Queue()))
    pass
    
    # åœ¨mainé‡Œé¢ä¸èƒ½ä½¿ç”¨await
    # asyncio.run(AItuber_novoice.add_token_and_check_sentence(",ä½ æ€ä¹ˆæ ·" , "ä½ å¥½ï¼"))
    

#  æµ‹è¯•æ˜¯å¦åœ¨è¿è¡Œæ—¶ä¿å­˜ä¿å­˜ï¼Œåœ¨åˆ‡æ¢çª—å£æ—¶