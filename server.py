# æ–‡ä»¶: /app/server.py
# æ ¸å¿ƒåŠŸèƒ½: å¯åŠ¨ FastAPI APIï¼Œåˆå§‹åŒ– Ollama LLM å’Œ BGE-M3 åµŒå…¥æ¨¡å‹ã€‚

import os
import sys
import logging
from typing import Optional

# ç¡®ä¿å¯¼å…¥äº†æ­£ç¡®çš„ LlamaIndex æ¨¡å—
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama as OllamaLLM
from llama_index.core.settings import Settings 

# å‡è®¾æ‚¨å·²å®‰è£… FastAPI
from fastapi import FastAPI, HTTPException

# ----------------------------------------------------
# 1. é…ç½®æ—¥å¿—ç³»ç»Ÿ
# ----------------------------------------------------
logging.basicConfig(level=logging.INFO, stream=sys.stdout)
logger = logging.getLogger(__name__)

# ----------------------------------------------------
# 2. æ ¸å¿ƒåˆå§‹åŒ–å‡½æ•° (åœ¨ FastAPI å¯åŠ¨å‰è¿è¡Œ)
# ----------------------------------------------------
def initialize_rag_components():
    """
    åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯ã€åµŒå…¥æ¨¡å‹ç­‰ RAG æ ¸å¿ƒç»„ä»¶ã€‚
    æ­¤å‡½æ•°å°†åœ¨ FastAPI åº”ç”¨ç¨‹åºå¯åŠ¨æ—¶è‡ªåŠ¨è°ƒç”¨ (on_startup)ã€‚
    """
    
    # å‡è®¾ Ollama æœåŠ¡è¿è¡Œåœ¨å®¹å™¨å†…éƒ¨çš„é»˜è®¤ç«¯å£
    OLLAMA_URL = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
    OLLAMA_EMBED_MODEL = "bge-m3"  # ç¡®ä¿ start.sh å·²ç» pull å¹¶é¢„çƒ­äº†è¿™ä¸ªæ¨¡å‹
    OLLAMA_LLM_MODEL = "llama3"    # ç¡®ä¿ start.sh å·²ç» pull äº†è¿™ä¸ªæ¨¡å‹

    # --- 1. é…ç½® Ollama åµŒå…¥æ¨¡å‹ (BGE-M3) ---
    try:
        logger.info(f"âœ… å°è¯•é…ç½® Ollama åµŒå…¥æ¨¡å‹: {OLLAMA_EMBED_MODEL}...")
        
        # ä½¿ç”¨ OllamaEmbedding ç±»ï¼Œé€šè¿‡ Ollama API è°ƒç”¨ BGE-M3
        embed_model = OllamaEmbedding(
            model_name=OLLAMA_EMBED_MODEL,
            base_url=OLLAMA_URL,
        )
        # å°† BGE-M3 è®¾ç½®ä¸º LlamaIndex çš„é»˜è®¤åµŒå…¥æ¨¡å‹
        Settings.embed_model = embed_model
        logger.info("âœ… BGE-M3 åµŒå…¥æ¨¡å‹å·²é€šè¿‡ Ollama API é…ç½®æˆåŠŸã€‚")
        
    except Exception as e:
        logger.error(f"âŒ Ollama åµŒå…¥æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        logger.error("è¯·ç¡®è®¤ 1. Ollama æœåŠ¡å·²å¯åŠ¨; 2. bge-m3 æ¨¡å‹å·²è¢«æ‹‰å–/é¢„çƒ­ã€‚")
        # å¦‚æœåµŒå…¥æ¨¡å‹é…ç½®å¤±è´¥ï¼ŒRAG æ— æ³•è¿›è¡Œï¼Œæ•…ç»ˆæ­¢ç¨‹åº
        sys.exit(1)
        
    # --- 2. é…ç½® Ollama LLM å®¢æˆ·ç«¯ ---
    try:
        logger.info(f"ğŸ”— é…ç½® Ollama LLM å®¢æˆ·ç«¯ ({OLLAMA_LLM_MODEL})...")
        llm_client = OllamaLLM(model=OLLAMA_LLM_MODEL, base_url=OLLAMA_URL)
        
        # å°† Ollama LLM è®¾ç½®ä¸º LlamaIndex çš„é»˜è®¤ LLM
        Settings.llm = llm_client
        logger.info(f"ğŸ”— Ollama LLM ({OLLAMA_LLM_MODEL}) å®¢æˆ·ç«¯é…ç½®æˆåŠŸã€‚")
        
    except Exception as e:
        # LLM å®¢æˆ·ç«¯é…ç½®å¤±è´¥ï¼Œè™½ç„¶ RAG ä¼šå—å½±å“ï¼Œä½†æˆ‘ä»¬æš‚ä¸ç»ˆæ­¢æœåŠ¡
        logger.error(f"âŒ Ollama LLM å®¢æˆ·ç«¯é…ç½®å¤±è´¥: {e}")
        
    # --- 3. æ ¸å¿ƒ RAG ç´¢å¼•åŠ è½½ (ç•™ç©ºå¾…å¡«å……) ---
    # è¿™é‡Œåº”è¯¥åŠ è½½æ‚¨çš„å‘é‡ç´¢å¼• (VectorStoreIndex)
    logger.info("ğŸš§ RAG æ ¸å¿ƒç´¢å¼•åŠ è½½é€»è¾‘å¾…å¡«å……ã€‚")
    
    logger.info("ğŸ‰ RAG æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–å®Œæˆã€‚")

# ----------------------------------------------------
# 3. FastAPI åº”ç”¨ç¨‹åºå¯åŠ¨
# ----------------------------------------------------

# ä½¿ç”¨ on_startup é’©å­ï¼Œç¡®ä¿ initialize_rag_components åœ¨æœåŠ¡å¯åŠ¨å‰è¿è¡Œ
app = FastAPI(
    on_startup=[initialize_rag_components],
    title="GPT-SoVITS & Ollama RAG Gateway"
) 

@app.get("/")
def read_root():
    """åŸºç¡€å¥åº·æ£€æŸ¥"""
    return {
        "status": "RAG API running", 
        "llm_configured": Settings.llm is not None,
        "embed_model_configured": Settings.embed_model is not None
    }

# ç¤ºä¾‹: å¾…å¡«å……çš„æŸ¥è¯¢è·¯ç”±
@app.post("/query")
def query_rag_endpoint(query_text: str):
    """å¤„ç†ç”¨æˆ·çš„ RAG æŸ¥è¯¢è¯·æ±‚ã€‚"""
    
    if Settings.llm is None:
         raise HTTPException(status_code=503, detail="LLM æœåŠ¡å°šæœªå°±ç»ªæˆ–é…ç½®å¤±è´¥ã€‚")
         
    # ğŸš§ å¾…å¡«å……: åœ¨è¿™é‡Œè°ƒç”¨ index.as_query_engine().query(query_text)
    
    return {"query": query_text, "response": "RAG é€»è¾‘å¾…å®ç°ã€‚"}


# ----------------------------------------------------
# 4. Uvicorn è¿è¡Œ (é€šå¸¸ç”± start.sh è„šæœ¬è°ƒç”¨)
# ----------------------------------------------------
# æ³¨æ„ï¼šåœ¨æ‚¨çš„ start.sh ä¸­ï¼Œæ‚¨æ˜¯ç›´æ¥è°ƒç”¨ python3 /app/server.py &
# ç¡®ä¿æ‚¨çš„å¯åŠ¨å‘½ä»¤æ˜¯æ­£ç¡®çš„ï¼Œä¾‹å¦‚ï¼š
# /venv_ollama/bin/uvicorn server:app --host 0.0.0.0 --port 8888
# å¦‚æœç›´æ¥è¿è¡Œ python3 /app/server.pyï¼Œåˆ™éœ€è¦åœ¨è„šæœ¬æœ«å°¾æ·»åŠ  uvicorn å¯åŠ¨é€»è¾‘