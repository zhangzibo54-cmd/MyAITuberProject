# æ–‡ä»¶: /app/server.py
# æ ¸å¿ƒåŠŸèƒ½: å¯åŠ¨ FastAPI APIï¼Œåˆå§‹åŒ– Ollama LLM å’Œ BGE-M3 åµŒå…¥æ¨¡å‹ã€‚

import os
import sys
import logging
from typing import Optional

import asyncio
import json
import wave
from io import BytesIO
from dataclasses import dataclass, field
from typing import List


# ç¡®ä¿å¯¼å…¥äº†æ­£ç¡®çš„ LlamaIndex æ¨¡å—
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama as OllamaLLM
from llama_index.core.settings import Settings 

import asyncio
import json
import wave
from io import BytesIO
from dataclasses import dataclass, field
from typing import List

# å‡è®¾æ‚¨å·²å®‰è£… FastAPI
from fastapi import FastAPI,HTTPException ,WebSocket, WebSocketDisconnect


from AIclass.events_class.utterance import UtteranceChunk 
from AIclass.aituber import AItuber
# ----------------------------------------------------
# 1. é…ç½®æ—¥å¿—ç³»ç»Ÿ
# ----------------------------------------------------
logging.basicConfig(level=logging.INFO, stream=sys.stdout)
logger = logging.getLogger(__name__)

# ----------------------------------------------------
# 2. æ ¸å¿ƒåˆå§‹åŒ–å‡½æ•° (åœ¨ FastAPI å¯åŠ¨å‰è¿è¡Œ)
# ----------------------------------------------------
def initialize_rag_components():
    """
    åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯ã€åµŒå…¥æ¨¡å‹ç­‰ RAG æ ¸å¿ƒç»„ä»¶ã€‚
    æ­¤å‡½æ•°å°†åœ¨ FastAPI åº”ç”¨ç¨‹åºå¯åŠ¨æ—¶è‡ªåŠ¨è°ƒç”¨ (on_startup)ã€‚
    """
    
    # å‡è®¾ Ollama æœåŠ¡è¿è¡Œåœ¨å®¹å™¨å†…éƒ¨çš„é»˜è®¤ç«¯å£
    OLLAMA_URL = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
    OLLAMA_EMBED_MODEL = "bge-m3"  # ç¡®ä¿ start.sh å·²ç» pull å¹¶é¢„çƒ­äº†è¿™ä¸ªæ¨¡å‹
    OLLAMA_LLM_MODEL = "llama3"    # ç¡®ä¿ start.sh å·²ç» pull äº†è¿™ä¸ªæ¨¡å‹

    # --- 1. é…ç½® Ollama åµŒå…¥æ¨¡å‹ (BGE-M3) ---
    try:
        logger.info(f"âœ… å°è¯•é…ç½® Ollama åµŒå…¥æ¨¡å‹: {OLLAMA_EMBED_MODEL}...")
        
        # ä½¿ç”¨ OllamaEmbedding ç±»ï¼Œé€šè¿‡ Ollama API è°ƒç”¨ BGE-M3
        embed_model = OllamaEmbedding(
            model_name=OLLAMA_EMBED_MODEL,
            base_url=OLLAMA_URL,
        )
        # å°† BGE-M3 è®¾ç½®ä¸º LlamaIndex çš„é»˜è®¤åµŒå…¥æ¨¡å‹
        Settings.embed_model = embed_model
        logger.info("âœ… BGE-M3 åµŒå…¥æ¨¡å‹å·²é€šè¿‡ Ollama API é…ç½®æˆåŠŸã€‚")
        
    except Exception as e:
        logger.error(f"âŒ Ollama åµŒå…¥æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        logger.error("è¯·ç¡®è®¤ 1. Ollama æœåŠ¡å·²å¯åŠ¨; 2. bge-m3 æ¨¡å‹å·²è¢«æ‹‰å–/é¢„çƒ­ã€‚")
        # å¦‚æœåµŒå…¥æ¨¡å‹é…ç½®å¤±è´¥ï¼ŒRAG æ— æ³•è¿›è¡Œï¼Œæ•…ç»ˆæ­¢ç¨‹åº
        sys.exit(1)
        
    # --- 2. é…ç½® Ollama LLM å®¢æˆ·ç«¯ ---
    try:
        logger.info(f"ğŸ”— é…ç½® Ollama LLM å®¢æˆ·ç«¯ ({OLLAMA_LLM_MODEL})...")
        llm_client = OllamaLLM(model=OLLAMA_LLM_MODEL, base_url=OLLAMA_URL)
        
        # å°† Ollama LLM è®¾ç½®ä¸º LlamaIndex çš„é»˜è®¤ LLM
        Settings.llm = llm_client
        logger.info(f"ğŸ”— Ollama LLM ({OLLAMA_LLM_MODEL}) å®¢æˆ·ç«¯é…ç½®æˆåŠŸã€‚")
        
    except Exception as e:
        # LLM å®¢æˆ·ç«¯é…ç½®å¤±è´¥ï¼Œè™½ç„¶ RAG ä¼šå—å½±å“ï¼Œä½†æˆ‘ä»¬æš‚ä¸ç»ˆæ­¢æœåŠ¡
        logger.error(f"âŒ Ollama LLM å®¢æˆ·ç«¯é…ç½®å¤±è´¥: {e}")
        
    # --- 3. æ ¸å¿ƒ RAG ç´¢å¼•åŠ è½½ (ç•™ç©ºå¾…å¡«å……) ---
    # è¿™é‡Œåº”è¯¥åŠ è½½æ‚¨çš„å‘é‡ç´¢å¼• (VectorStoreIndex)
    logger.info("ğŸš§ RAG æ ¸å¿ƒç´¢å¼•åŠ è½½é€»è¾‘å¾…å¡«å……ã€‚")
    
    logger.info("ğŸ‰ RAG æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–å®Œæˆã€‚")

# ----------------------------------------------------
# 3. FastAPI åº”ç”¨ç¨‹åºå¯åŠ¨
# ----------------------------------------------------

# ä½¿ç”¨ on_startup é’©å­ï¼Œç¡®ä¿ initialize_rag_components åœ¨æœåŠ¡å¯åŠ¨å‰è¿è¡Œ
app = FastAPI(
    on_startup=[initialize_rag_components],
    title="GPT-SoVITS, Ollama RAG Gateway and send text_audio"
) 

@app.get("/")
def read_root():
    """åŸºç¡€å¥åº·æ£€æŸ¥"""
    return {
        "status": "RAG API running", 
        "llm_configured": Settings.llm is not None,
        "embed_model_configured": Settings.embed_model is not None
    }

# ç¤ºä¾‹: å¾…å¡«å……çš„æŸ¥è¯¢è·¯ç”±
@app.post("/query")
def query_rag_endpoint(query_text: str):
    """å¤„ç†ç”¨æˆ·çš„ RAG æŸ¥è¯¢è¯·æ±‚ã€‚"""
    
    if Settings.llm is None:
         raise HTTPException(status_code=503, detail="LLM æœåŠ¡å°šæœªå°±ç»ªæˆ–é…ç½®å¤±è´¥ã€‚")
         
    # ğŸš§ å¾…å¡«å……: åœ¨è¿™é‡Œè°ƒç”¨ index.as_query_engine().query(query_text)
    
    return {"query": query_text, "response": "RAG é€»è¾‘å¾…å®ç°ã€‚"}


@app.websocket("/ws/stream_uuerances")
async def websocket_endpoint(websocket: WebSocket, utterance_list: asyncio.Queue):
    #å®Œæˆtext_audioçš„ä¼ è¾“
    await websocket.accept()
    print("å®¢æˆ·ç«¯å·²ç»è¿æ¥")

    utterance_list = asyncio.Queue()
    # to put aituber
    aituber = None

    aituber_task =  asyncio.create_task( AItuber.main(text_audio_queue = utterance_list,aituber = aituber) )
    await asyncio.gather(aituber_task)
    #å½“utteranceæ²¡æœ‰é‡åˆ°æˆªè‡³ä¿¡å·æ—¶ï¼Œä¸ä¼š
    try:
        while True:
                #è¿™é‡Œè€—æ—¶ä¸Šä¼ ä»»åŠ¡ä»ç„¶ç”¨await asyncio.to_thread()
                #æ³¨æ„è¿™é‡Œwait forä»ç„¶éœ€è¦await
                #é•¿æ—¶é—´
                chunk = await asyncio.wait_for(utterance_list.get(),timeout=100000)
                metadata = chunk.to_dict()
                
                
                await  websocket.send_bytes(chunk.audio_data)
                await  websocket.send_text(json.dumps(metadata))

                print(f"å·²å‘é€{metadata["text"]}å’Œæ–‡æœ¬")
                pass
    except KeyboardInterrupt:
        print("å› ä¸ºè¾“å…¥CTRL + Cè€Œç»ˆæ­¢äº†server.py")
    except  Exception as e:
        print(f"æœåŠ¡å™¨ä¼ è¾“æ•°æ®ç»™æœ¬åœ°æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    finally:
        aituber.stop_consciousness()
        aituber_task.cancel()
        try:
            await aituber_task
        except asyncio.CancelledError:
            logger.info(f"âœ… ä¸ºå®¢æˆ·ç«¯åˆ›å»ºçš„ AItuber åå°ä»»åŠ¡å·²æˆåŠŸæ¸…ç†ã€‚")

        logger.info(f"å®¢æˆ·ç«¯çš„ä¼šè¯å·²å®Œå…¨ç»“æŸã€‚")


if __name__ =="__main__":
    import uvicorn
    # è¿è¡ŒæœåŠ¡å™¨ï¼Œç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£(æ‰€æœ‰äººéƒ½èƒ½è¯·æ±‚)çš„ 8000ï¼ˆè®¿é—®åœ°å€porthttp://<æœåŠ¡å™¨IP>:8000ï¼‰ ç«¯å£  
    uvicorn.run(app, host="0.0.0.0", port=8000)
  



# ----------------------------------------------------
# 4. Uvicorn è¿è¡Œ (é€šå¸¸ç”± start.sh è„šæœ¬è°ƒç”¨)
# ----------------------------------------------------
# æ³¨æ„ï¼šåœ¨æ‚¨çš„ start.sh ä¸­ï¼Œæ‚¨æ˜¯ç›´æ¥è°ƒç”¨ python3 /app/server.py &
# ç¡®ä¿æ‚¨çš„å¯åŠ¨å‘½ä»¤æ˜¯æ­£ç¡®çš„ï¼Œä¾‹å¦‚ï¼š
# /venv_ollama/bin/uvicorn server:app --host 0.0.0.0 --port 8888
# å¦‚æœç›´æ¥è¿è¡Œ python3 /app/server.pyï¼Œåˆ™éœ€è¦åœ¨è„šæœ¬æœ«å°¾æ·»åŠ  uvicorn å¯åŠ¨é€»è¾‘