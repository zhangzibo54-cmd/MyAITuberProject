# æ–‡ä»¶: /app/server.py
# æ ¸å¿ƒåŠŸèƒ½: å¯åŠ¨ FastAPI APIï¼Œåˆå§‹åŒ– Ollama LLM å’Œ BGE-M3 åµŒå…¥æ¨¡å‹ã€‚

import os
import sys
import logging
from typing import Optional

import asyncio
import json
import wave
from io import BytesIO
from dataclasses import dataclass, field
from typing import List


# ç¡®ä¿å¯¼å…¥äº†æ­£ç¡®çš„ LlamaIndex æ¨¡å—
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama as OllamaLLM
from llama_index.core.settings import Settings 

import asyncio
import json
import wave
from io import BytesIO
from dataclasses import dataclass, field
from typing import List

# å‡è®¾æ‚¨å·²å®‰è£… FastAPI
from fastapi import FastAPI,HTTPException ,WebSocket, WebSocketDisconnect

import numpy as np
import librosa
from faster_whisper import WhisperModel


from AIclass.aituber import AItuber
from AIclass.events_class.perception_events import PerceptionEvent
from AIclass.events_class.utterance import UtteranceChunk
from AIclass.sub_engines.perception_engine import PerceptionEngine as PEG

# ----------------------------------------------------
# 1. é…ç½®æ—¥å¿—ç³»ç»Ÿ
# ----------------------------------------------------
logging.basicConfig(level=logging.INFO, stream=sys.stdout)
logger = logging.getLogger(__name__)

asr_model = None # <<<< æ–°å¢ï¼šå…¨å±€å˜é‡ç”¨äºæŒæœ‰ASRæ¨¡å‹
# ----------------------------------------------------
# 2. æ ¸å¿ƒåˆå§‹åŒ–å‡½æ•° (åœ¨ FastAPI å¯åŠ¨å‰è¿è¡Œ)
# ----------------------------------------------------
asr_model = None

def initialize_rag_components():
    """
    åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯ã€åµŒå…¥æ¨¡å‹ç­‰ RAG æ ¸å¿ƒç»„ä»¶ã€‚
    æ­¤å‡½æ•°å°†åœ¨ FastAPI åº”ç”¨ç¨‹åºå¯åŠ¨æ—¶è‡ªåŠ¨è°ƒç”¨ (on_startup)ã€‚
    """
    
    # å‡è®¾ Ollama æœåŠ¡è¿è¡Œåœ¨å®¹å™¨å†…éƒ¨çš„é»˜è®¤ç«¯å£
    OLLAMA_URL = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
    OLLAMA_EMBED_MODEL = "bge-m3"  # ç¡®ä¿ start.sh å·²ç» pull å¹¶é¢„çƒ­äº†è¿™ä¸ªæ¨¡å‹
    OLLAMA_LLM_MODEL = "llama3"    # ç¡®ä¿ start.sh å·²ç» pull äº†è¿™ä¸ªæ¨¡å‹

    # --- 1. é…ç½® Ollama åµŒå…¥æ¨¡å‹ (BGE-M3) ---
    try:
        logger.info(f"âœ… å°è¯•é…ç½® Ollama åµŒå…¥æ¨¡å‹: {OLLAMA_EMBED_MODEL}...")
        
        # ä½¿ç”¨ OllamaEmbedding ç±»ï¼Œé€šè¿‡ Ollama API è°ƒç”¨ BGE-M3
        embed_model = OllamaEmbedding(
            model_name=OLLAMA_EMBED_MODEL,
            base_url=OLLAMA_URL,
        )
        # å°† BGE-M3 è®¾ç½®ä¸º LlamaIndex çš„é»˜è®¤åµŒå…¥æ¨¡å‹
        Settings.embed_model = embed_model
        logger.info("âœ… BGE-M3 åµŒå…¥æ¨¡å‹å·²é€šè¿‡ Ollama API é…ç½®æˆåŠŸã€‚")
        
    except Exception as e:
        logger.error(f"âŒ Ollama åµŒå…¥æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        logger.error("è¯·ç¡®è®¤ 1. Ollama æœåŠ¡å·²å¯åŠ¨; 2. bge-m3 æ¨¡å‹å·²è¢«æ‹‰å–/é¢„çƒ­ã€‚")
        # å¦‚æœåµŒå…¥æ¨¡å‹é…ç½®å¤±è´¥ï¼ŒRAG æ— æ³•è¿›è¡Œï¼Œæ•…ç»ˆæ­¢ç¨‹åº
        sys.exit(1)
        
    # --- 2. é…ç½® Ollama LLM å®¢æˆ·ç«¯ ---
    try:
        logger.info(f"ğŸ”— é…ç½® Ollama LLM å®¢æˆ·ç«¯ ({OLLAMA_LLM_MODEL})...")
        llm_client = OllamaLLM(model=OLLAMA_LLM_MODEL, base_url=OLLAMA_URL)
        
        # å°† Ollama LLM è®¾ç½®ä¸º LlamaIndex çš„é»˜è®¤ LLM
        Settings.llm = llm_client
        logger.info(f"ğŸ”— Ollama LLM ({OLLAMA_LLM_MODEL}) å®¢æˆ·ç«¯é…ç½®æˆåŠŸã€‚")
        
    except Exception as e:
        # LLM å®¢æˆ·ç«¯é…ç½®å¤±è´¥ï¼Œè™½ç„¶ RAG ä¼šå—å½±å“ï¼Œä½†æˆ‘ä»¬æš‚ä¸ç»ˆæ­¢æœåŠ¡
        logger.error(f"âŒ Ollama LLM å®¢æˆ·ç«¯é…ç½®å¤±è´¥: {e}")
        
    # --- 3. æ ¸å¿ƒ RAG ç´¢å¼•åŠ è½½ (ç•™ç©ºå¾…å¡«å……) ---
    # è¿™é‡Œåº”è¯¥åŠ è½½æ‚¨çš„å‘é‡ç´¢å¼• (VectorStoreIndex)
    logger.info("ğŸš§ RAG æ ¸å¿ƒç´¢å¼•åŠ è½½é€»è¾‘å¾…å¡«å……ã€‚")
    
    logger.info("ğŸ‰ RAG æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–å®Œæˆã€‚")

    # --- 4. æ–°å¢ï¼šåˆå§‹åŒ– ASR æ¨¡å‹ ---
    try:
        model_size = "large-v3"
        logger.info(f"âœ… æ­£åœ¨åŠ è½½ ASR æ¨¡å‹ (faster-whisper: {model_size})...")
        # ä½¿ç”¨å…¨å±€å˜é‡
        global asr_model
        asr_model = WhisperModel(model_size, device="cuda", compute_type="float16")
        logger.info(f"âœ… ASR æ¨¡å‹ ({model_size}) åŠ è½½æˆåŠŸå¹¶å·²å‡†å¤‡å°±ç»ªã€‚")
    except Exception as e:
        logger.error(f"âŒ ASR æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        logger.error("è¯·ç¡®ä¿ faster-whisper å’Œç›¸å…³ä¾èµ–å·²æ­£ç¡®å®‰è£…åœ¨ venv_ollama ç¯å¢ƒä¸­ã€‚")
        # ASR å¯¹äºæ­¤åº”ç”¨è‡³å…³é‡è¦ï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™é€€å‡º
        sys.exit(1)

# ----------------------------------------------------
# 3. FastAPI åº”ç”¨ç¨‹åºå¯åŠ¨
# ----------------------------------------------------

# ä½¿ç”¨ on_startup é’©å­ï¼Œç¡®ä¿ initialize_rag_components åœ¨æœåŠ¡å¯åŠ¨å‰è¿è¡Œ
app = FastAPI(
    on_startup=[initialize_rag_components],
    title="GPT-SoVITS, Ollama RAG Gateway and send-get text_audio"
) 

@app.get("/")
def read_root():
    """åŸºç¡€å¥åº·æ£€æŸ¥"""
    return {
        "status": "RAG API running", 
        "llm_configured": Settings.llm is not None,
        "embed_model_configured": Settings.embed_model is not None
    }

# ç¤ºä¾‹: å¾…å¡«å……çš„æŸ¥è¯¢è·¯ç”±
@app.post("/query")
def query_rag_endpoint(query_text: str):
    """å¤„ç†ç”¨æˆ·çš„ RAG æŸ¥è¯¢è¯·æ±‚ã€‚"""
    
    if Settings.llm is None:
         raise HTTPException(status_code=503, detail="LLM æœåŠ¡å°šæœªå°±ç»ªæˆ–é…ç½®å¤±è´¥ã€‚")
         
    # ğŸš§ å¾…å¡«å……: åœ¨è¿™é‡Œè°ƒç”¨ index.as_query_engine().query(query_text)
    
    return {"query": query_text, "response": "RAG é€»è¾‘å¾…å®ç°ã€‚"}


@app.websocket("/ws/stream_utterances")
async def websocket_endpoint(websocket: WebSocket):
    #å®Œæˆtext_audioçš„ä¼ è¾“
    await websocket.accept()
    client_host = websocket.client.host
    print(f"å®¢æˆ·ç«¯ {client_host} å·²è¿æ¥ã€‚")

    # --- ä¸ºè¯¥å®¢æˆ·ç«¯ä¼šè¯åˆå§‹åŒ–èµ„æº ---
    # è¿™ä¸ªé˜Ÿåˆ—ç”¨äºæ¥æ”¶ AITuber ç”Ÿæˆçš„è¦æ’­æ”¾çš„è¯­éŸ³
    output_utterance_queue = asyncio.Queue()
    # è¿™ä¸ªé˜Ÿåˆ—ç”¨äºå­˜æ”¾è¯†åˆ«å‡ºçš„æ–‡æœ¬ï¼Œå¹¶ä¼ é€’ç»™ AITuber
    perception_event_queue = asyncio.Queue()
    aituber_task = None
    audio_receiver_task = None
    try:
        aituber_task = asyncio.create_task(
                AItuber.main(text_audio_queue=output_utterance_queue,
                             asr_model = asr_model)
            )
        async def receive_audio_handler():
            try:
                while True:
                    # ç­‰å¾…å¹¶æ¥æ”¶æ¥è‡ªå®¢æˆ·ç«¯çš„äºŒè¿›åˆ¶æ•°æ®
                    audio_bytes = await websocket.receive_bytes()
                    await  PEG.put_audio_queue(audio_data= audio_bytes)
                    print(f"æœåŠ¡å™¨: æ”¶åˆ°æ¥è‡ªå®¢æˆ·ç«¯çš„éŸ³é¢‘æ•°æ®å—ï¼Œé•¿åº¦: {len(audio_bytes)}")
                    
                    # ----------------------------------------------------
                    # TODO: ä¹‹ååœ¨è¿™é‡Œæ·»åŠ è¯­éŸ³è½¬æ–‡æœ¬çš„é€»è¾‘ã€‚
                    # ASR å¤„ç†åçš„æ–‡æœ¬ï¼Œå°†é€šè¿‡ AITuber å†…éƒ¨çš„
                    # perception_event_queue ä¼ é€’ç»™ PerceptionEngineã€‚
                    # ----------------------------------------------------
            except Exception as e:
                    # å½“å®¢æˆ·ç«¯æ–­å¼€æ—¶ï¼Œè¿™ä¸ªæ¥æ”¶å¾ªç¯ä¼šæ­£å¸¸ç»“æŸ
                    print("éŸ³é¢‘æ¥æ”¶ä»»åŠ¡åœæ­¢ã€‚")
        audio_receiver_task = asyncio.create_task(receive_audio_handler())
            
        while True:
            chunk = await output_utterance_queue.get()
            if chunk is None:  # å‡è®¾ None æ˜¯ç»“æŸä¿¡å·
                break
            
            metadata = chunk.to_dict()
            await websocket.send_text(json.dumps(metadata))
            if chunk.audio_data:
                await websocket.send_bytes(chunk.audio_data)
    except KeyboardInterrupt:
        print("å› ä¸ºè¾“å…¥CTRL + Cè€Œç»ˆæ­¢äº†server.py")
    except WebSocketDisconnect:
        # å®¢æˆ·ç«¯æ–­å¼€è¿æ¥æ—¶ï¼Œä¼šè§¦å‘è¿™ä¸ªå¼‚å¸¸
        print("å®¢æˆ·ç«¯æ–­å¼€ï¼ŒéŸ³é¢‘æ¥æ”¶ä»»åŠ¡åœæ­¢ã€‚")
    except  Exception as e:
        print(f"æœåŠ¡å™¨ä¼ è¾“æ•°æ®ç»™æœ¬åœ°æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    finally:
        if audio_receiver_task and not audio_receiver_task.done():
            audio_receiver_task.cancel()
        if aituber_task and not aituber_task.done():
            aituber_task.cancel()
        await asyncio.gather(aituber_task, audio_receiver_task, return_exceptions=True)

        logger.info(f"å®¢æˆ·ç«¯çš„ä¼šè¯å·²å®Œå…¨ç»“æŸã€‚")


if __name__ =="__main__":
    import uvicorn
    # è¿è¡ŒæœåŠ¡å™¨ï¼Œç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£(æ‰€æœ‰äººéƒ½èƒ½è¯·æ±‚)çš„ 8888ï¼ˆè®¿é—®åœ°å€porthttp://<æœåŠ¡å™¨IP>:8000ï¼‰ ç«¯å£  
    uvicorn.run(app, host="0.0.0.0", port=8888)
  



# ----------------------------------------------------
# 4. Uvicorn è¿è¡Œ (é€šå¸¸ç”± start.sh è„šæœ¬è°ƒç”¨)
# ----------------------------------------------------
# æ³¨æ„ï¼šåœ¨æ‚¨çš„ start.sh ä¸­ï¼Œæ‚¨æ˜¯ç›´æ¥è°ƒç”¨ python3 /app/server.py &
# ç¡®ä¿æ‚¨çš„å¯åŠ¨å‘½ä»¤æ˜¯æ­£ç¡®çš„ï¼Œä¾‹å¦‚ï¼š
# /venv_ollama/bin/uvicorn server:app --host 0.0.0.0 --port 8888
# å¦‚æœç›´æ¥è¿è¡Œ python3 /app/server.pyï¼Œåˆ™éœ€è¦åœ¨è„šæœ¬æœ«å°¾æ·»åŠ  uvicorn å¯åŠ¨é€»è¾‘