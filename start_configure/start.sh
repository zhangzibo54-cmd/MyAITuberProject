
#!/bin/bash

# --- 1. é…ç½®æ¨¡å‹å’Œè·¯å¾„ ---

export MPLBACKEND='agg'

SCRIPT_DIR=$(dirname "$0")
REQUIREMENTS_FILE_GPTS="${SCRIPT_DIR}/requirements_gpts.txt"
REQUIREMENTS_FILE_OLLAMA="${SCRIPT_DIR}/requirements_ollama.txt"
# ç¡®ä¿æ‰€æœ‰ GPT-SoVITS å­æ¨¡å—è·¯å¾„è¢«è¯†åˆ«
export PYTHONPATH="/app:/app/GPT-SoVITS:/app/GPT-SoVITS/GPT_SoVITS:/app/GPT-SoVITS/GPT_SOVITS/module:/app/GPT-SoVITS/GPT_SOVITS/eres2net:$PYTHONPATH"

XINGTONG_FOLDER="/app/XingTong"
PRETRAINED_FOLDER="/app/GPT-SoVITS/GPT_SoVITS/pretrained_models" 
API_PY_PATH="/app/GPT-SoVITS/api.py"

SOVITS_PATH="${XINGTONG_FOLDER}/sovits.pth"
GPT_PATH="${XINGTONG_FOLDER}/gpt.ckpt"
HUBERT_PATH="${PRETRAINED_FOLDER}/chinese-hubert-base"
BERT_PATH="${PRETRAINED_FOLDER}/chinese-roberta-wwm-ext-large"

DEFAULT_REF_WAV_PATH="${XINGTONG_FOLDER}/ref.wav"
DEFAULT_REF_TEXT="ç­‰ä½ ï¼Œæˆ‘æƒ³æƒ³ï¼Œå—¯ã€‚"
DEFAULT_REF_LANG="zh"
GPT_SOVITS_PORT=9880
BIND_ADDRESS="0.0.0.0"

VENV_GPTS="/app/venv_gpts"
VENV_OLLAMA="/app/venv_ollama"

echo "ğŸ› ï¸ æ­£åœ¨ä¿®å¤ GPT-SoVITS æ¨¡å—å¯¼å…¥è·¯å¾„ (PYTHONPATH)..."
export PYTHONPATH="/app:/app/GPT-SoVITS:/app/GPT-SoVITS/GPT_SoVITS:/app/GPT-SoVITS/GPT_SOVITS/module:/app/GPT-SoVITS/GPT_SOVITS/eres2net:$PYTHONPATH"

# --- 1. SSHæœåŠ¡å’Œæ¸…ç† ---
echo "ğŸš€ å¯åŠ¨ SSH æœåŠ¡..."
/usr/sbin/sshd -D &

echo "ğŸ§¹ æ­£åœ¨æ¸…ç†å¯èƒ½æ®‹ç•™çš„æ—§æœåŠ¡å™¨è¿›ç¨‹..."
fuser -k 9880/tcp || true
fuser -k 11434/tcp || true
fuser -k 8888/tcp || true
sleep 1

# --- 2. æ ¸å¿ƒï¼šæ£€æŸ¥å¹¶å®‰è£… Python è™šæ‹Ÿç¯å¢ƒ ---
if [ ! -d "${VENV_GPTS}" ]; then
    echo "ğŸ“¦ æ­£åœ¨å®‰è£… GPT-SoVITS Python è™šæ‹Ÿç¯å¢ƒ..."
    python3 -m venv ${VENV_GPTS}
    ${VENV_GPTS}/bin/python3 -m pip install --upgrade pip
    # ä» PyTorch å®˜æ–¹æºåœ¨çº¿å®‰è£…ï¼ŒæŒ‡å®š CUDA 12.1 ç‰ˆæœ¬
    # ã€ä¿®æ”¹ç‚¹ã€‘å»æ‰ç‰ˆæœ¬å·ï¼Œè®© pip è‡ªåŠ¨é€‰æ‹©å…¼å®¹ç‰ˆæœ¬
    ${VENV_GPTS}/bin/python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    ${VENV_GPTS}/bin/python3 -m pip install --no-cache-dir -r "${REQUIREMENTS_FILE_GPTS}"
fi

if [ ! -d "${VENV_OLLAMA}" ]; then
    echo "ğŸ“¦ æ­£åœ¨å®‰è£… Ollama/API Python è™šæ‹Ÿç¯å¢ƒ..."
    python3 -m venv ${VENV_OLLAMA}
    ${VENV_OLLAMA}/bin/python3 -m pip install --upgrade pip
    # ä» PyTorch å®˜æ–¹æºåœ¨çº¿å®‰è£…ï¼ŒæŒ‡å®š CUDA 12.1 ç‰ˆæœ¬
    # ã€ä¿®æ”¹ç‚¹ã€‘å»æ‰ç‰ˆæœ¬å·ï¼Œè®© pip è‡ªåŠ¨é€‰æ‹©å…¼å®¹ç‰ˆæœ¬
    ${VENV_GPTS}/bin/python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    # ç»§ç»­å®‰è£…å…¶ä»–ä¾èµ–
    ${VENV_OLLAMA}/bin/python3 -m pip install --no-cache-dir -r "${REQUIREMENTS_FILE_OLLAMA}"
fi
# æ·»åŠ ç¼“å­˜æ–‡ä»¶å¤¹ï¼Œä¸€ä¸ªGPTSçš„ä¿®å¤
FASTLANG_CACHE_PATH="/app/GPT-SoVITS/GPT_SoVITS/pretrained_models/fast_langdetect"
echo "ğŸ“‚ æ­£åœ¨ç¡®ä¿ fast_langdetect ç¼“å­˜è·¯å¾„å­˜åœ¨: ${FASTLANG_CACHE_PATH}"
mkdir -p "${FASTLANG_CACHE_PATH}"

echo "âš™ï¸ æ­£åœ¨æ‰§è¡Œ API ä¿®å¤è¡¥ä¸ (patch_api.py)..."
/venv_gpts/bin/python3 "${SCRIPT_DIR}/patch_api.py"

# --- 3. å¯åŠ¨æœåŠ¡ ---
echo "=== æ­£åœ¨æ£€æŸ¥ nvcc æ˜¯å¦å®‰è£…,ä¼¼ä¹æ²¡æœ‰å¿…è¦ï¼Œä¹‹å‰æ˜¯ä¸ºäº†whisperçš„ä¿®å¤æ‰å®‰è£…çš„ï¼Œä½†å®é™…é—®é¢˜æ˜¯duicrtranslateçš„ç‰ˆæœ¬ä¾èµ–==="
if command -v nvcc &> /dev/null
then
    echo "nvcc å·²å®‰è£…ã€‚è·³è¿‡å®‰è£…æ­¥éª¤ã€‚"
else
    echo "nvcc æœªæ‰¾åˆ°ã€‚å¼€å§‹å®‰è£… CUDA Toolkit..."
    
    # æ£€æŸ¥å¹¶å®‰è£…åŸºæœ¬å·¥å…·
    apt update
    apt install -y wget dpkg ca-certificates

    # --- æ­¥éª¤ 1: æ·»åŠ  NVIDIA ä»“åº“ ---
    echo "ä¸‹è½½å¹¶å®‰è£… CUDA å¯†é’¥åŒ…..."
    if wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb -O /tmp/cuda-keyring.deb; then
        dpkg -i /tmp/cuda-keyring.deb
        rm /tmp/cuda-keyring.deb
    else
        echo "CUDA å¯†é’¥ä¸‹è½½å¤±è´¥ (404 æˆ–å…¶ä»–é”™è¯¯)ã€‚è¯·æ£€æŸ¥ URLã€‚"
        exit 1
    fi

    # --- æ­¥éª¤ 2: å®‰è£… CUDA Toolkit ---
    echo "æ›´æ–° APT åˆ—è¡¨å¹¶å®‰è£… cuda-toolkit..."
    apt update
    apt install -y cuda-toolkit
    
    # ç¡®ä¿ PATH å˜é‡åœ¨å½“å‰ä¼šè¯ä¸­è®¾ç½®
    export PATH="/usr/local/cuda/bin:$PATH"
    
    echo "=== å®‰è£…å®Œæˆ ==="
fi


echo "ğŸŒ æ£€æŸ¥å¹¶ä¸‹è½½ NLTK è¯­è¨€æ•°æ®åŒ…..."
source ${VENV_GPTS}/bin/activate
python3 -c "import nltk; nltk_packages = ['averaged_perceptron_tagger', 'punkt', 'cmudict', 'averaged_perceptron_tagger_eng']; [nltk.download(package, quiet=True) for package in nltk_packages]"
deactivate
echo "âœ… NLTKæ•°æ®åŒ…å·²å‡†å¤‡å°±ç»ªã€‚"

echo "ğŸš€ æ¿€æ´» GPT-SoVITS ç¯å¢ƒå¹¶ä¿®å¤è·¯å¾„..."
source ${VENV_GPTS}/bin/activate
export PYTHONPATH="/app:/app/GPT-SoVITS:/app/GPT-SoVITS/GPT_SoVITS:/app/GPT-SoVITS/GPT_SOVITS/module:/app/GPT-SoVITS/GPT_SOVITS/eres2net:$PYTHONPATH"

if [ "$IS_REMOTE_SERVER" = "true" ]; then
    echo "ğŸš€ æ£€æµ‹åˆ°è¿œç¨‹æœåŠ¡å™¨ç¯å¢ƒã€‚å¯åŠ¨ GPT-SoVITS API æœåŠ¡å™¨..."
    (
    #é˜²æ­¢æ‰¾ä¸åˆ°æ–‡ä»¶æ‰§è¡Œï¼Œå…¶ä¸­ï¼ˆï¼‰ä»£è¡¨å†…éƒ¨æ‰§è¡Œå‘½ä»¤ï¼Œåœ¨æ‰§è¡Œç»“æŸåè‡ªåŠ¨å›åˆ°åŸæ–‡ä»¶å¤¹
    source ${VENV_GPTS}/bin/activate
    export PYTHONPATH="/app:/app/GPT-SoVITS:/app/GPT-SoVITS/GPT_SoVITS:/app/GPT-SoVITS/GPT_SOVITS/module:/app/GPT-SoVITS/GPT_SOVITS/eres2net:$PYTHONPATH"
    cd /app/GPT-SoVITS
    ${VENV_GPTS}/bin/python3 "${API_PY_PATH}" \
        -s "${SOVITS_PATH}" \
        -g "${GPT_PATH}" \
        -hb "${HUBERT_PATH}" \
        -b "${BERT_PATH}" \
        -dr "${DEFAULT_REF_WAV_PATH}" \
        -dt "${DEFAULT_REF_TEXT}" \
        -dl "${DEFAULT_REF_LANG}" \
        -a "${BIND_ADDRESS}" \
        -p "${GPT_SOVITS_PORT}" \
        -d "cuda" &
    )
else
    echo "âš ï¸ æœªæ£€æµ‹åˆ°è¿œç¨‹æœåŠ¡å™¨æ ‡å¿—ã€‚è·³è¿‡å¯åŠ¨ GPT-SoVITS API æœåŠ¡å™¨ã€‚"
fi

if [ "$IS_REMOTE_SERVER" = "true" ]; then
    echo "ğŸš€ 2/3: å¯åŠ¨ Ollama æœåŠ¡å’Œ LLM æ¨¡å‹..."

    echo "ğŸ”„ æ­£åœ¨æ£€æŸ¥å¹¶å®‰è£…/æ›´æ–° Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
    # âœ… æ–°å¢ï¼šå¼ºåˆ¶ Ollama ä½¿ç”¨ GPU
    # ã€å…³é”®ä¿®æ­£ã€‘ä½¿ç”¨ find å‘½ä»¤æ‰¾åˆ°çš„ç²¾ç¡®è·¯å¾„æ¥è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œ
    # ä¹‹å‰çš„è·¯å¾„ä¸å¯¹è¿™ä¸ªæˆ‘ä»¬é€šè¿‡æŸ¥æ‰¾ find / -name "libcublas.so*" 2>/dev/nullæ‰¾åˆ°çš„
    # ã€æœ€ç»ˆä¿®æ­£ç‰ˆã€‘æ ¹æ®ç³»ç»ŸçœŸå®æ–‡ä»¶ç»“æ„å®šåˆ¶çš„è·¯å¾„ï¼Œ
    #   å®é™…ä¸Šåœ¨é‡‡ç”¨ä¸Šè¿°å®‰è£…ollamaçš„æ–¹æ³•åï¼Œä¸éœ€è¦å†è®¾ç½®è¿™ä¸ªè·¯å¾„äº†
    # export LD_LIBRARY_PATH="/usr/local/cuda-12.1/targets/x86_64-linux/lib:/usr/local/cuda-12.1/lib64:${LD_LIBRARY_PATH}"

    # åå°å¯åŠ¨ollamaæœåŠ¡ ---
    /usr/local/bin/ollama serve &

    echo "â±ï¸ ç­‰å¾… Ollama æœåŠ¡å¯åŠ¨..."
    sleep 10

    OLLAMA_MODEL_ID="llama3:latest"
    OLLAMA_EMBED_MODEL="bge-m3:latest"

    echo "â¬‡ï¸ æ£€æŸ¥å¹¶æ‹‰å– Ollama æ¨¡å‹: ${OLLAMA_MODEL_ID}..."
    if ! /usr/local/bin/ollama list | grep -q "${OLLAMA_MODEL_ID}"; then
        /usr/local/bin/ollama pull "${OLLAMA_MODEL_ID}" &> /dev/null
        /usr/local/bin/ollama run "${OLLAMA_MODEL_ID}" "hello" &> /dev/null
    fi

    echo "â¬‡ï¸ æ£€æŸ¥å¹¶æ‹‰å– Ollama åµŒå…¥æ¨¡å‹: ${OLLAMA_EMBED_MODEL}..."
    if ! /usr/local/bin/ollama list | grep -q "${OLLAMA_EMBED_MODEL}"; then
        /usr/local/bin/ollama pull "${OLLAMA_EMBED_MODEL}" &> /dev/null
        echo "ğŸ”¥ æ­£åœ¨é¢„çƒ­ ${OLLAMA_EMBED_MODEL}..."
        /usr/local/bin/ollama run "${OLLAMA_EMBED_MODEL}" "é¢„çƒ­" &> /dev/null
    fi

    source ${VENV_OLLAMA}/bin/activate
    echo "ğŸš€ å¯åŠ¨ä¸»ç¨‹åº server.py (LLM/API é€»è¾‘)..."
    # è¿™é‡Œæ—¶ç”¨unicornè¿è¡Œçš„æ‰€ä»¥éœ€è¦ç›¸å¯¹è·¯å¾„è€Œéç»å¯¹è·¯å¾„
    # MODULE_PATH="start_configure.server:app"
    # ${VENV_OLLAMA}/bin/uvicorn ${MODULE_PATH} --host 0.0.0.0 --port 8888 &

    ${VENV_OLLAMA}/bin/uvicorn server:app --host 0.0.0.0 --port 8888 &
    deactivate
    echo "âœ… Ollama æœåŠ¡å’Œæ¨¡å‹å’Œserver.pyçš„æ‰§è¡Œå·²å‡†å¤‡å®Œæ¯•."
else
    echo "âš ï¸ Ollama å’Œ LLM/RAG API å¯åŠ¨è·³è¿‡ (éè¿œç¨‹ç¯å¢ƒ)."
fi

# --- 4. ä¿æŒå®¹å™¨æŒç»­è¿è¡Œ ---
echo "âœ… æ‰€æœ‰æœåŠ¡å¯åŠ¨å®Œæ¯•ï¼Œä¿æŒå®¹å™¨è¿è¡Œ..."
tail -f /dev/null
