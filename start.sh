
#!/bin/bash

# --- 1. é…ç½®æ¨¡å‹å’Œè·¯å¾„ ---

export MPLBACKEND='agg'

# ç¡®ä¿æ‰€æœ‰ GPT-SoVITS å­æ¨¡å—è·¯å¾„è¢«è¯†åˆ«
export PYTHONPATH="/app:/app/GPT-SoVITS:/app/GPT-SoVITS/GPT_SoVITS:/app/GPT-SoVITS/GPT_SOVITS/module:/app/GPT-SoVITS/GPT_SOVITS/eres2net:$PYTHONPATH"

XINGTONG_FOLDER="/app/XingTong"
PRETRAINED_FOLDER="/app/GPT-SoVITS/GPT_SoVITS/pretrained_models" 
API_PY_PATH="/app/GPT-SoVITS/api.py"

SOVITS_PATH="${XINGTONG_FOLDER}/sovits.pth"
GPT_PATH="${XINGTONG_FOLDER}/gpt.ckpt"
HUBERT_PATH="${PRETRAINED_FOLDER}/chinese-hubert-base"
BERT_PATH="${PRETRAINED_FOLDER}/chinese-roberta-wwm-ext-large"

DEFAULT_REF_WAV_PATH="${XINGTONG_FOLDER}/ref.wav"
DEFAULT_REF_TEXT="ç­‰ä½ ï¼Œæˆ‘æƒ³æƒ³ï¼Œå—¯ã€‚"
DEFAULT_REF_LANG="zh"
GPT_SOVITS_PORT=9880
BIND_ADDRESS="0.0.0.0"

VENV_GPTS="/venv_gpts"
VENV_OLLAMA="/venv_ollama"

echo "ğŸ› ï¸ æ­£åœ¨ä¿®å¤ GPT-SoVITS æ¨¡å—å¯¼å…¥è·¯å¾„ (PYTHONPATH)..."
export PYTHONPATH="/app:/app/GPT-SoVITS:/app/GPT-SoVITS/GPT_SoVITS:/app/GPT-SoVITS/GPT_SOVITS/module:/app/GPT-SoVITS/GPT_SOVITS/eres2net:$PYTHONPATH"

# --- 1. SSHæœåŠ¡å’Œæ¸…ç† ---
echo "ğŸš€ å¯åŠ¨ SSH æœåŠ¡..."
/usr/sbin/sshd -D &

echo "ğŸ§¹ æ­£åœ¨æ¸…ç†å¯èƒ½æ®‹ç•™çš„æ—§æœåŠ¡å™¨è¿›ç¨‹ (ç«¯å£ 9880)..."
fuser -k 9880/tcp || true
sleep 1

# --- 2. æ ¸å¿ƒï¼šæ£€æŸ¥å¹¶å®‰è£… Python è™šæ‹Ÿç¯å¢ƒ ---
if [ ! -d "${VENV_GPTS}" ]; then
    echo "ğŸ“¦ æ­£åœ¨å®‰è£… GPT-SoVITS Python è™šæ‹Ÿç¯å¢ƒ..."
    python3 -m venv ${VENV_GPTS}
    ${VENV_GPTS}/bin/python3 -m pip install --upgrade pip
    # ä» PyTorch å®˜æ–¹æºåœ¨çº¿å®‰è£…ï¼ŒæŒ‡å®š CUDA 12.1 ç‰ˆæœ¬
    ${VENV_GPTS}/bin/python3 -m pip install torch==2.1.0 torchvision==0.17.2 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu121
    # ç»§ç»­å®‰è£…å…¶ä»–ä¾èµ–
    ${VENV_GPTS}/bin/python3 -m pip install --no-cache-dir -r requirements_gpts.txt
fi

if [ ! -d "${VENV_OLLAMA}" ]; then
    echo "ğŸ“¦ æ­£åœ¨å®‰è£… Ollama/API Python è™šæ‹Ÿç¯å¢ƒ..."
    python3 -m venv ${VENV_OLLAMA}
    ${VENV_OLLAMA}/bin/python3 -m pip install --upgrade pip
    # ä» PyTorch å®˜æ–¹æºåœ¨çº¿å®‰è£…ï¼ŒæŒ‡å®š CUDA 12.1 ç‰ˆæœ¬
    ${VENV_GPTS}/bin/python3 -m pip install torch==2.1.0 torchvision==0.17.2 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu121
    # ç»§ç»­å®‰è£…å…¶ä»–ä¾èµ–
    ${VENV_OLLAMA}/bin/python3 -m pip install --no-cache-dir -r requirements_ollama.txt
fi

FASTLANG_CACHE_PATH="/app/GPT-SoVITS/GPT_SoVITS/pretrained_models/fast_langdetect"
echo "ğŸ“‚ æ­£åœ¨ç¡®ä¿ fast_langdetect ç¼“å­˜è·¯å¾„å­˜åœ¨: ${FASTLANG_CACHE_PATH}"
mkdir -p "${FASTLANG_CACHE_PATH}"

echo "âš™ï¸ æ­£åœ¨æ‰§è¡Œ API ä¿®å¤è¡¥ä¸ (patch_api.py)..."
/venv_gpts/bin/python3 /app/patch_api.py

# --- 3. å¯åŠ¨æœåŠ¡ ---
echo "ğŸŒ æ£€æŸ¥å¹¶ä¸‹è½½ NLTK è¯­è¨€æ•°æ®åŒ…..."
source ${VENV_GPTS}/bin/activate
python3 -c "import nltk; nltk_packages = ['averaged_perceptron_tagger', 'punkt', 'cmudict', 'averaged_perceptron_tagger_eng']; [nltk.download(package, quiet=True) for package in nltk_packages]"
deactivate
echo "âœ… NLTKæ•°æ®åŒ…å·²å‡†å¤‡å°±ç»ªã€‚"

echo "ğŸš€ æ¿€æ´» GPT-SoVITS ç¯å¢ƒå¹¶ä¿®å¤è·¯å¾„..."
source ${VENV_GPTS}/bin/activate
export PYTHONPATH="/app:/app/GPT-SoVITS:/app/GPT-SoVITS/GPT_SoVITS:/app/GPT-SoVITS/GPT_SOVITS/module:/app/GPT-SoVITS/GPT_SOVITS/eres2net:$PYTHONPATH"

if [ "$IS_REMOTE_SERVER" = "true" ]; then
    echo "ğŸš€ æ£€æµ‹åˆ°è¿œç¨‹æœåŠ¡å™¨ç¯å¢ƒã€‚å¯åŠ¨ GPT-SoVITS API æœåŠ¡å™¨..."
    (
    source ${VENV_GPTS}/bin/activate
    export PYTHONPATH="/app:/app/GPT-SoVITS:/app/GPT-SoVITS/GPT_SoVITS:/app/GPT-SoVITS/GPT_SOVITS/module:/app/GPT-SoVITS/GPT_SOVITS/eres2net:$PYTHONPATH"
    cd /app/GPT-SoVITS
    ${VENV_GPTS}/bin/python3 "${API_PY_PATH}" \
        -s "${SOVITS_PATH}" \
        -g "${GPT_PATH}" \
        -hb "${HUBERT_PATH}" \
        -b "${BERT_PATH}" \
        -dr "${DEFAULT_REF_WAV_PATH}" \
        -dt "${DEFAULT_REF_TEXT}" \
        -dl "${DEFAULT_REF_LANG}" \
        -a "${BIND_ADDRESS}" \
        -p "${GPT_SOVITS_PORT}" \
        -d "cuda" &
    )
else
    echo "âš ï¸ æœªæ£€æµ‹åˆ°è¿œç¨‹æœåŠ¡å™¨æ ‡å¿—ã€‚è·³è¿‡å¯åŠ¨ GPT-SoVITS API æœåŠ¡å™¨ã€‚"
fi

if [ "$IS_REMOTE_SERVER" = "true" ]; then
    echo "ğŸš€ 2/3: å¯åŠ¨ Ollama æœåŠ¡å’Œ LLM æ¨¡å‹..."

    # âœ… æ–°å¢ï¼šå¼ºåˆ¶ Ollama ä½¿ç”¨ GPU
    export LD_LIBRARY_PATH="/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib/x86_64-linux-gnu:${PYTHONPATH}:${LD_LIBRARY_PATH}"
    /usr/local/bin/ollama serve & 
    echo "â±ï¸ ç­‰å¾… Ollama æœåŠ¡å¯åŠ¨..."
    sleep 20

    OLLAMA_MODEL_ID="llama3:latest"
    OLLAMA_EMBED_MODEL="bge-m3:latest"

    echo "â¬‡ï¸ æ£€æŸ¥å¹¶æ‹‰å– Ollama æ¨¡å‹: ${OLLAMA_MODEL_ID}..."
    if ! /usr/local/bin/ollama list | grep -q "${OLLAMA_MODEL_ID}"; then
        /usr/local/bin/ollama pull "${OLLAMA_MODEL_ID}" &> /dev/null
    fi

    echo "â¬‡ï¸ æ£€æŸ¥å¹¶æ‹‰å– Ollama åµŒå…¥æ¨¡å‹: ${OLLAMA_EMBED_MODEL}..."
    if ! /usr/local/bin/ollama list | grep -q "${OLLAMA_EMBED_MODEL}"; then
        /usr/local/bin/ollama pull "${OLLAMA_EMBED_MODEL}" &> /dev/null
        echo "ğŸ”¥ æ­£åœ¨é¢„çƒ­ ${OLLAMA_EMBED_MODEL}..."
        /usr/local/bin/ollama run "${OLLAMA_EMBED_MODEL}" "é¢„çƒ­" &> /dev/null
    fi

    source ${VENV_OLLAMA}/bin/activate
    echo "ğŸš€ å¯åŠ¨ä¸»ç¨‹åº server.py (LLM/API é€»è¾‘)..."
    ${VENV_OLLAMA}/bin/python3 /app/server.py &
    deactivate
    echo "âœ… Ollama æœåŠ¡å’Œæ¨¡å‹å’Œserver.pyçš„æ‰§è¡Œå·²å‡†å¤‡å®Œæ¯•."
else
    echo "âš ï¸ Ollama å’Œ LLM/RAG API å¯åŠ¨è·³è¿‡ (éè¿œç¨‹ç¯å¢ƒ)."
fi

# --- 4. ä¿æŒå®¹å™¨æŒç»­è¿è¡Œ ---
echo "âœ… æ‰€æœ‰æœåŠ¡å¯åŠ¨å®Œæ¯•ï¼Œä¿æŒå®¹å™¨è¿è¡Œ..."
tail -f /dev/null
